{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook makes vector figure as examples to include in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import erf\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 9}\n",
    "plt.rc('font', **font)\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "from IPython.display import display, clear_output\n",
    "import time as systime\n",
    "sys.path.append('../')\n",
    "import DataProcessing.utils.TwoDimTTC as TwoDimTTC\n",
    "from DataProcessing.utils.coortrans import coortrans\n",
    "coortrans = coortrans()\n",
    "from visual_utils import *\n",
    "from GaussianProcessRegression.training_utils import *\n",
    "\n",
    "path_raw = '../Data/RawData/'\n",
    "path_processed = '../Data/ProcessedData/'\n",
    "path_input = '../Data/InputData/'\n",
    "path_output = '../Data/OutputData/'\n",
    "fig_path = r'C:/SURFdrive/PhD progress/PhDResearch/4_Conflict/AMAR/Figures/'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "if device=='cpu':\n",
    "    num_threads = torch.get_num_threads()\n",
    "    print(f'Number of available threads: {num_threads}')\n",
    "    torch.set_num_threads(round(num_threads/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 5\n",
    "num_inducing_points = 100\n",
    "model_idx = 53\n",
    "pipeline = train_val_test(device, num_inducing_points, path_input, path_output)\n",
    "pipeline.model.load_state_dict(torch.load(path_output+f'trained_models/highD_LC/beta={beta}/model_{model_idx}epoch.pth', map_location=torch.device(device)))\n",
    "pipeline.likelihood.load_state_dict(torch.load(path_output+f'trained_models/highD_LC/beta={beta}/likelihood_{model_idx}epoch.pth', map_location=torch.device(device)))\n",
    "pipeline.model.eval()\n",
    "pipeline.likelihood.eval()\n",
    "model = pipeline.model.to(device)\n",
    "likelihood = pipeline.likelihood.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Near-crashes 100Car NDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "events = pd.read_hdf(path_output + 'conflict_probability/optimal_warning/NearCrashes.h5', key='data')\n",
    "meta = pd.read_csv(path_output + 'conflict_probability/optimal_warning/Unified_NearCrashes.csv').set_index('trip_id')\n",
    "proximity_phi = pd.read_csv(path_output+'conflict_probability/proximity_phi.csv')\n",
    "\n",
    "n = 13\n",
    "trip_id = 8332\n",
    "t = -4.4 # -4.4 and -2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = events[events['trip_id']==trip_id].sort_values('time').reset_index(drop=True)\n",
    "df = df.merge(proximity_phi, on=['trip_id','time'])\n",
    "df['s_centroid'] = np.sqrt((df['x_i'] - df['x_j']) ** 2 + (df['y_i'] - df['y_j']) ** 2)\n",
    "df['probability'] = extreme_cdf(df['s_centroid'].values, df['mu'].values, df['sigma'].values, n)\n",
    "moment = meta.loc[trip_id]['moment']\n",
    "df['time'] = np.round(df['time'] - moment, 2)\n",
    "conflict_start = df[df['event']]['time'].min()\n",
    "conflict_end = df[df['event']]['time'].max()\n",
    "\n",
    "df['delta_v'] = np.sqrt((df['vx_i']-df['vx_j'])**2 + (df['vy_i']-df['vy_j'])**2)\n",
    "df['delta_v2'] = df['delta_v']**2\n",
    "df['speed_i2'] = df['speed_i']**2\n",
    "df['speed_j2'] = df['speed_j']**2\n",
    "features = ['length_i','length_j','hx_j','hy_j','delta_v','delta_v2','speed_i2','speed_j2','acc_i','rho']\n",
    "df_view_i = coortrans.transform_coor(df, view='i')\n",
    "heading_j = df_view_i[['time','hx_j','hy_j']]\n",
    "df_view_i = df_view_i.set_index('time')\n",
    "df_relative = coortrans.transform_coor(df, view='relative')\n",
    "rho = coortrans.angle(1, 0, df_relative['x_j'], df_relative['y_j']).reset_index().rename(columns={0:'rho'})\n",
    "rho['time'] = df_relative['time']\n",
    "df_relative = df_relative.set_index('time')\n",
    "interaction_situation = df.drop(columns=['hx_j','hy_j']).merge(heading_j, on='time').merge(rho, on='time')\n",
    "interaction_situation = interaction_situation[features+['time']].set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = viual_100Car(t, df, df_view_i, df_relative, interaction_situation, \n",
    "                   model, likelihood, device, n, conflict_start, conflict_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(fig_path+f'ProbEst_{trip_id}_{round(t*100)}.pdf', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lane-changes highD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatafiles =  sorted(glob.glob(path_raw + 'highD/RecordingMetadata/*.csv'))\n",
    "metadata = []\n",
    "for metadatafile in metadatafiles:\n",
    "    df = pd.read_csv(metadatafile)\n",
    "    metadata.append(df)\n",
    "metadata = pd.concat(metadata).set_index('id')\n",
    "\n",
    "def ricker_wavelet_gradient(x, a=10):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of smoothed Ricker wavelet function\n",
    "\n",
    "    Parameters:\n",
    "    x (float): The lateral velocities at which to evaluate the wavelet.\n",
    "    a (float): The width parameter of the wavelet. Default is 10Hz as we have downsampled highD data to 10fps.\n",
    "    \"\"\"\n",
    "    coefficient = 2/(np.sqrt(3*a)*(np.pi**0.25))\n",
    "    ricker_wavelet = coefficient * (1 - (x/a)**2) * np.exp(-0.5*(x/a)**2)\n",
    "    smoothed_wavelet = savgol_filter(ricker_wavelet, 30, 3)\n",
    "    gradient = np.gradient(smoothed_wavelet)\n",
    "    return gradient\n",
    "\n",
    "loc_id = 1\n",
    "veh_id_i = 291860\n",
    "veh_id_j = 291858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(path_processed+'highD/highD_'+str(loc_id).zfill(2)+'.h5', key='data')\n",
    "data = data.sort_values(['track_id','frame_id']).set_index('track_id')\n",
    "\n",
    "meta_tracks = []\n",
    "for fileid in metadata[metadata['locationId']==loc_id].index:\n",
    "    meta_track = pd.read_csv(path_raw+'highD/01_tracksMeta.csv')\n",
    "    meta_track['id'] = (fileid*10000 + meta_track['id']).astype(int)\n",
    "    meta_tracks.append(meta_track)\n",
    "meta_tracks = pd.concat(meta_tracks)\n",
    "\n",
    "lane_markings = [float(y) for lane in ['lowerLaneMarkings','upperLaneMarkings'] for y in metadata.loc[veh_id_i//10000][lane].split(';')]\n",
    "lane_markings = np.sort(lane_markings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_i = data.loc[veh_id_i]\n",
    "lane_change_points = veh_i[abs(veh_i['laneId'].diff())>0]['frame_id'].values\n",
    "frame_segments = np.concatenate(([veh_i['frame_id'].min()], \n",
    "                                 (lane_change_points[1:] + lane_change_points[:-1])/2, \n",
    "                                 [veh_i['frame_id'].max()]))\n",
    "frame_start = frame_segments[0]\n",
    "frame_end = frame_segments[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_i = data.loc[veh_id_i]\n",
    "veh_i = veh_i[veh_i['frame_id'].between(frame_start, frame_end)].drop(columns=['laneId','precedingId', 'followingId'])\n",
    "veh_j = data.loc[veh_id_j].drop(columns=['laneId','precedingId', 'followingId'])\n",
    "df = veh_i.merge(veh_j, on='frame_id', suffixes=('_i', '_j'), how='inner')\n",
    "df['time'] = np.round((df['frame_id'] - df['frame_id'].min())/10, 1)\n",
    "df['delta_v'] = np.sqrt((df['vx_i']-df['vx_j'])**2 + (df['vy_i']-df['vy_j'])**2)\n",
    "df['delta_v2'] = df['delta_v']**2\n",
    "df['speed_i2'] = df['speed_i']**2\n",
    "df['speed_j2'] = df['speed_j']**2\n",
    "df['acc_i'] = df['ax_i']*df['hx_i'] + df['ay_i']*df['hy_i']\n",
    "df['s_centroid'] = np.sqrt((df['x_i']-df['x_j'])**2 + (df['y_i']-df['y_j'])**2)\n",
    "df['TTC'] = TwoDimTTC.TTC(df, 'values')\n",
    "\n",
    "other_vehs = data[(data['frame_id']>=df['frame_id'].min())&\n",
    "                    (data['frame_id']<=df['frame_id'].max())&\n",
    "                    (data.index!=veh_id_i)].reset_index()\n",
    "df_view_i, other_vehs_view_i = coortrans.TransCoorVis(df.set_index('frame_id').copy(), other_vehs.set_index('frame_id').copy(), relative=False)\n",
    "df_relative = coortrans.transform_coor(df, view='relative')\n",
    "heading_j = df_view_i.reset_index()[['frame_id','hx_j','hy_j']]\n",
    "rho = coortrans.angle(1, 0, df_relative['x_j'], df_relative['y_j']).reset_index().rename(columns={0:'rho'})\n",
    "rho['frame_id'] = df_relative['frame_id']\n",
    "features = ['length_i','length_j','hx_j','hy_j','delta_v','delta_v2','speed_i2','speed_j2','acc_i','rho']\n",
    "interaction_situation = df.drop(columns=['hx_j','hy_j']).merge(heading_j, on='frame_id').merge(rho, on='frame_id')\n",
    "interaction_situation = interaction_situation[features+['frame_id']].set_index('frame_id')\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    f_dist = model(torch.Tensor(interaction_situation.values).to(device))\n",
    "    y_dist = likelihood(f_dist)\n",
    "    mu_list, sigma_list = y_dist.mean.cpu().numpy(), y_dist.variance.sqrt().cpu().numpy()\n",
    "df['mu'] = mu_list\n",
    "df['sigma'] = sigma_list\n",
    "df['n_hat'] = np.log(0.5)/np.log(1-lognormal_cdf(df['s_centroid'], df['mu'], df['sigma']))\n",
    "df.loc[df['n_hat']<1, 'n_hat'] = 1\n",
    "\n",
    "wavelet = ricker_wavelet_gradient(abs(veh_i['vy']))\n",
    "frame_1 = veh_i.iloc[wavelet.argmin()]['frame_id']\n",
    "frame_2 = veh_i.iloc[wavelet.argmax()]['frame_id']\n",
    "if frame_1 > frame_2:\n",
    "    frame_1, frame_2 = frame_2, frame_1\n",
    "df = df[(df['frame_id']>=frame_1-30)&(df['frame_id']<=frame_2+30)]\n",
    "\n",
    "print(df['frame_id'].min(), df['frame_id'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameid = 2909763 # 2909719 and 2909750\n",
    "\n",
    "fig = visual_highD(lane_markings, frameid, veh_i, veh_j, df, df_view_i, other_vehs, other_vehs_view_i, \n",
    "                   interaction_situation, model, likelihood, device, frame_1, frame_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(fig_path+f'IntEva_{loc_id}_{veh_id_i}_{veh_id_j}_{frameid}.pdf', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video images for dynamic Figure 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path = path_output + 'video_images/Figure9/'\n",
    "for frameid in tqdm(df['frame_id'].unique()):\n",
    "    fig = visual_highD(lane_markings, frameid, veh_i, veh_j, df, df_view_i, other_vehs, other_vehs_view_i, \n",
    "                    interaction_situation, model, likelihood, device, frame_1, frame_2)\n",
    "    fig.savefig(fig_path+f'{loc_id}_{veh_id_i}_{veh_id_j}_{frameid}.png', dpi=600, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conflict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
